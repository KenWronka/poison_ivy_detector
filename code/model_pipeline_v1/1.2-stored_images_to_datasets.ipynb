{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets\n",
    "\n",
    "Use pre-prepared image downloads to create datasets with train/test splits for model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sqa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from typing import Dict, Tuple\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from enum import Enum, auto\n",
    "from numbers import Number\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.9\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# Database parameters\n",
    "db_container = \"metadata_db\"\n",
    "db_user = \"pguser\"\n",
    "db_password = \"pgpassword\"\n",
    "db_port = 5432\n",
    "db_database = \"metadata\"\n",
    "db_prefix = \"postgresql\"\n",
    "\n",
    "metadata_tbl = \"base_images\"\n",
    "datasets_tbl = \"datasets\"\n",
    "dataset_img_tbl = \"dataset_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_con_str = f\"{db_prefix}://{db_user}:{db_password}@{db_container}:{db_port}/{db_database}\"\n",
    "db_engine = sqa.create_engine(db_con_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Label_Method(Enum):\n",
    "    BINARY: 'Image_Label_Method' = auto()\n",
    "    MULTI: 'Image_Label_Method' = auto()\n",
    "\n",
    "@dataclass\n",
    "class Image_Dataset_Config:\n",
    "    \"\"\"\n",
    "    Specifies config for creating an image dataset:\n",
    "     - name: name of the dataset\n",
    "     - target_dir: directory for storing the dataset\n",
    "     - description\n",
    "     - validation frac: frac of TRAINING data to use as training dataset (if 0, no validation set created)\n",
    "     - test_frac: frac of ALL data to use as testing data (if 0, no test set created)\n",
    "     - label_method: different labelling methods implemented:\n",
    "         - BINARY: All labels just positive or negative\n",
    "         - MULTI: Each datapoint is labelled by its source\n",
    "     - label_counts: Dictionary of ((label str, source_name), count) pairs - how many data points from each data source to use\n",
    "        (if count is -1 or is greater than total available, just use all of them)\n",
    "        \n",
    "    \"\"\"\n",
    "    name: str\n",
    "    target_dir: str\n",
    "    label_counts: Dict[Tuple[str, str], int] = field(default_factory=dict)\n",
    "    class_names: Dict[Tuple[str, str], str] = field(default_factory=dict)\n",
    "    description: str = \"\"\n",
    "    validation_frac: float = 0.2\n",
    "    test_frac: float = float(0)\n",
    "    label_method: Image_Label_Method = Image_Label_Method.BINARY\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        assert len(self.label_counts) > 0, \\\n",
    "            \"Must include counts for at least one source + label\"\n",
    "        assert self.label_counts.keys() == self.class_names.keys(), \\\n",
    "            \"Must include class name to match each class label count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset_Builder():\n",
    "    \"\"\"\n",
    "    Class for creating a dataset from a metadata table and configs\n",
    "    db_engine: database engine attached to metadata database\n",
    "    dataset_config: specifies how to create dataset (see Image_Dataset_Config class)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 db_engine: sqa.engine.Engine,\n",
    "                 config: Image_Dataset_Config,\n",
    "                 random_state: int = 101,\n",
    "                 db_metadata_tbl: str = metadata_tbl,\n",
    "                 db_datasets_tbl: str = datasets_tbl,\n",
    "                 db_dataset_img_tbl: str = dataset_img_tbl,\n",
    "                 ):\n",
    "        self.db_metadata_tbl = db_metadata_tbl\n",
    "        self.db_datasets_tbl = db_datasets_tbl\n",
    "        self.db_dataset_img_tbl = db_dataset_img_tbl\n",
    "        self.config = config\n",
    "        self.engine = db_engine\n",
    "        self.random_state = random_state   # For sampling\n",
    "        with self.engine.connect() as con:\n",
    "            self.df_img: pd.DataFrame = pd.read_sql(self.db_metadata_tbl, con, index_col='image_name')\n",
    "            self.df_img = self.df_img[self.df_img['read'] == True]  # We only care about usable images now\n",
    "        self._create_dataset_dfs()\n",
    "    \n",
    "    \n",
    "    # Use config to create dataframe of desired image outputs\n",
    "    def _create_dataset_dfs(self):\n",
    "        # Datasets dataframe\n",
    "        self.df_ds = pd.DataFrame([\n",
    "            asdict(self.config)\n",
    "        ]).set_index(\"name\").drop(['label_counts', 'class_names'], axis=1)\n",
    "        self.df_ds['label_method'] = self.df_ds['label_method'].map(lambda lm: lm.name)\n",
    "        \n",
    "        # Dataset Images dataframe\n",
    "        self.df_ds_img = self.df_img \\\n",
    "            .groupby(['label_str', 'source']) \\\n",
    "            .apply(lambda df: df.sample(0 if df.name not in self.config.label_counts.keys()\n",
    "                                        else len(df.index) if (self.config.label_counts[df.name] == -1)\n",
    "                                        else min(len(df.index), self.config.label_counts[df.name]),\n",
    "                                        replace=False, random_state=self.random_state)) \\\n",
    "            [[]].reset_index().set_index('image_name')\n",
    "        self.df_ds_img['class_name'] = self.df_ds_img.apply(\n",
    "            lambda row: self.config.class_names[(row['label_str'], row['source'])], axis=1)\n",
    "        \n",
    "        if self.config.label_method == Image_Label_Method.BINARY:\n",
    "            self.df_ds_img['class_label'] = self.df_ds_img['label_str']\n",
    "        elif self.config.label_method == Image_Label_Method.MULTI:\n",
    "            self.df_ds_img['class_label'] = self.df_ds_img['class_name']\n",
    "        else:\n",
    "            self.df_ds_img['class_label'] = self.df_ds_img['label_str']\n",
    "        label_value_dict = {label: i for i, label in enumerate(self.df_ds_img['class_label'].unique())}\n",
    "        self.df_ds_img['class_value'] = self.df_ds_img['class_label'].map(label_value_dict)\n",
    "        self.df_ds_img['dataset_name'] = self.config.name\n",
    "        self.df_ds_img['dataset_img_path'] = None\n",
    "        self.df_ds_img['split'] = None\n",
    "        self.df_ds_img = self.df_ds_img.drop(['label_str', 'source'], axis=1)\n",
    "        \n",
    "    \n",
    "    # Assign each data point to a train/test/validation split\n",
    "    def _assign_splits(self):\n",
    "        train_X = self.df_ds_img.index\n",
    "        val_X = []\n",
    "        test_X = []\n",
    "        \n",
    "        if self.config.test_frac > 0:\n",
    "            train_X, test_X = train_test_split(self.df_ds_img.index,\n",
    "                                               test_size=self.config.test_frac,\n",
    "                                               stratify=self.df_ds_img['class_label'],\n",
    "                                               random_state=self.random_state)\n",
    "        if self.config.validation_frac > 0:\n",
    "            train_X, val_X = train_test_split(self.df_ds_img.index,\n",
    "                                              test_size=self.config.validation_frac,\n",
    "                                              stratify=self.df_ds_img['class_label'],\n",
    "                                              random_state=self.random_state)\n",
    "        \n",
    "        self.df_ds_img.loc[train_X, 'split'] = 'train'\n",
    "        self.df_ds_img.loc[test_X, 'split'] = 'test'\n",
    "        self.df_ds_img.loc[val_X, 'split'] = 'validation'\n",
    "        \n",
    "        self.df_ds_img['dataset_img_path'] = self.df_ds_img.apply(\n",
    "            lambda row: str(Path(self.config.target_dir,\n",
    "                                 row['split'],\n",
    "                                 row['class_label'],\n",
    "                                 row.name + '.jpg')),\n",
    "        axis=1)\n",
    "        \n",
    "    # Clear data associated with this dataset from dataset (or create tables if they don't exist)\n",
    "    def _clear_db_data(self):\n",
    "        with self.engine.connect() as con:\n",
    "            db_ds_tbl_exists = con.execute(self._query_check_tbl_exists(\n",
    "                self.db_datasets_tbl)).fetchall()[0][0]\n",
    "            db_ds_img_tbl_exists = con.execute(self._query_check_tbl_exists(\n",
    "                self.db_dataset_img_tbl)).fetchall()[0][0]\n",
    "                \n",
    "            \n",
    "            if db_ds_tbl_exists:\n",
    "                con.execute(self._query_drop_datasets_rows)\n",
    "            else:\n",
    "                con.execute(self._query_create_dataset_tbl)\n",
    "            if not db_ds_img_tbl_exists:\n",
    "                con.execute(self._query_create_dataset_img_tbl)\n",
    "    \n",
    "    # Create (if not exists) or clear (if exists) data from target directory\n",
    "    def _clear_target_dir(self):\n",
    "        if os.path.exists(self.config.target_dir):\n",
    "            for root, dirs, files in os.walk(self.config.target_dir):\n",
    "                for f in files:\n",
    "                    os.unlink(os.path.join(root, f))\n",
    "                for d in dirs:\n",
    "                    shutil.rmtree(os.path.join(root, d))\n",
    "        else:\n",
    "            Path(self.config.target_dir).mkdir(parents=True)\n",
    "        \n",
    "        for split in self.df_ds_img['split'].unique():\n",
    "            for class_label in self.df_ds_img['class_label']:\n",
    "                Path(self.config.target_dir, split, class_label).mkdir(exist_ok=True, parents=True)\n",
    "            \n",
    "    \n",
    "    # Copy files from image dataframe from source to target directory\n",
    "    def _copy_image_files(self):\n",
    "        for img_name, row in self.df_ds_img.iterrows():\n",
    "            shutil.copyfile(self.df_img.loc[img_name, 'final_loc'],\n",
    "                            row['dataset_img_path'])\n",
    "    \n",
    "    # Store associated data to the metadata db (dataset table and dataset images table)\n",
    "    def _store_db_metadata(self):\n",
    "        with self.engine.connect() as con:\n",
    "            self.df_ds.to_sql(self.db_datasets_tbl, con, if_exists='append')\n",
    "            self.df_ds_img.to_sql(self.db_dataset_img_tbl, con, if_exists='append')\n",
    "    \n",
    "    \n",
    "    # Builds the dataset by running the other methods\n",
    "    def build_dataset(self):\n",
    "        success_flag = True\n",
    "        self._assign_splits()\n",
    "        logging.info(\"Splits assigned\")\n",
    "        self._clear_db_data()\n",
    "        logging.info(\"Database cleared\")\n",
    "        self._clear_target_dir()\n",
    "        logging.info(\"Directory cleared\")\n",
    "        try:\n",
    "            self._copy_image_files()\n",
    "            logging.info(\"Images copied\")\n",
    "            self._store_db_metadata()\n",
    "            logging.info(\"Data stored to db\")\n",
    "        except:\n",
    "            logging.error(\"Dataset copy unsuccessful, removing data from db and directory\")\n",
    "            success_flag = False\n",
    "            self._clear_db_data()\n",
    "            self._clear_target_dir()\n",
    "        return(success_flag)\n",
    "            \n",
    "    \n",
    "    \n",
    "    # QUERY HELPERS\n",
    "    @property\n",
    "    def _query_create_dataset_tbl(self):\n",
    "        return(f\"\"\"\n",
    "               CREATE TABLE {self.db_datasets_tbl} (\n",
    "                   name VARCHAR(100) UNIQUE NOT NULL,\n",
    "                   target_dir VARCHAR(300) NOT NULL,\n",
    "                   description TEXT,\n",
    "                   validation_frac NUMERIC NOT NULL,\n",
    "                   test_frac NUMERIC NOT NULL,\n",
    "                   label_method VARCHAR(50) NOT NULL,\n",
    "                   PRIMARY KEY(name)\n",
    "               )\n",
    "               \"\"\")\n",
    "    @property\n",
    "    def _query_create_dataset_img_tbl(self):\n",
    "        return(f\"\"\"\n",
    "               CREATE TABLE {self.db_dataset_img_tbl} (\n",
    "                   image_name VARCHAR(300) NOT NULL,\n",
    "                   dataset_name VARCHAR(100) NOT NULL,\n",
    "                   class_name VARCHAR(100) NOT NULL,\n",
    "                   class_label VARCHAR(100) NOT NULL,\n",
    "                   class_value INT NOT NULL,\n",
    "                   split VARCHAR(30) NOT NULL,\n",
    "                   dataset_img_path VARCHAR(500) NOT NULL,\n",
    "                   PRIMARY KEY (image_name, dataset_name),\n",
    "                   FOREIGN KEY (image_name) REFERENCES {self.db_metadata_tbl}(image_name),\n",
    "                   FOREIGN KEY (dataset_name) REFERENCES {self.db_datasets_tbl}(name) ON DELETE CASCADE\n",
    "               )\n",
    "               \"\"\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def _query_check_tbl_exists(tbl_name):\n",
    "        return(f\"\"\"\n",
    "                SELECT EXISTS(\n",
    "                    SELECT * FROM information_schema.tables\n",
    "                    WHERE table_name = '{tbl_name}'\n",
    "                )\n",
    "               \"\"\")\n",
    "    @property \n",
    "    def _query_datasets_exists(self):\n",
    "        return(self._query_check_tbl_exists(self.db_datasets_tbl))\n",
    "    @property \n",
    "    def _query_dataset_imgs_exists(self):\n",
    "        return(self._query_check_tbl_exists(self.db_dataset_img_tbl))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _query_drop_col_values_from_tbl(tbl_name, col_name, col_value):\n",
    "        return(f\"\"\"\n",
    "               DELETE FROM {tbl_name}\n",
    "               WHERE {col_name} = {(\"'\" + col_value + \"'\") if isinstance(col_value, str) else col_value}\n",
    "               \"\"\")\n",
    "    @property \n",
    "    def _query_drop_datasets_rows(self):\n",
    "        return(self._query_drop_col_values_from_tbl(self.db_datasets_tbl, \"name\", self.config.name))\n",
    "    @property \n",
    "    def _query_drop_dataset_imgs_rows(self):\n",
    "        return(self._query_drop_col_values_from_tbl(self.db_dataset_img_tbl,\n",
    "                                                    \"image_name\",\n",
    "                                                    self.config.name))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a\n",
       "1    (1, 3)\n",
       "4    (4, 6)\n",
       "dtype: object"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([\n",
    "    (1,2,3),\n",
    "    (4,5,6)\n",
    "], columns=['a', 'b', 'c']).set_index('a')\n",
    "df.apply(lambda row: (row.name, row['c']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('negative', 'Google Images'),\n",
       " ('negative', 'Imagenet'),\n",
       " ('negative', 'Plantnet'),\n",
       " ('positive', 'Google Images')}"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with db_engine.connect() as con:\n",
    "    cols = pd.read_sql_query(f\"SELECT source, label_str FROM {metadata_tbl}\", con)\n",
    "    unique_src_label_pairs = set(\n",
    "        tuple(record) for record in pd.read_sql_query(f\"SELECT label_str, source FROM {metadata_tbl}\",\n",
    "                                                      con).to_records(index=False)\n",
    "    )\n",
    "    # sources = [*pd.read_sql_query(f\"SELECT source FROM {metadata_tbl}\", con)['source'].unique()]\n",
    "unique_src_label_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splits assigned\n",
      "INFO:root:Database cleared\n",
      "INFO:root:Directory cleared\n",
      "INFO:root:Images copied\n",
      "INFO:root:Data stored to db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bin_label_counts = {pair: 50 for pair in unique_src_label_pairs}\n",
    "test_bin_label_names = {\n",
    "    ('negative', 'Google Images'): \"negative_similar_plant\",\n",
    "    ('negative', 'Imagenet'): \"negative_random_picture\",\n",
    "    ('negative', 'Plantnet'): \"negative_general_plant\",\n",
    "    ('positive', 'Google Images'): \"positive\",\n",
    "}\n",
    "test_bin_config = Image_Dataset_Config(\n",
    "    name=\"test_bin_example\",\n",
    "    target_dir=\"../datasets/pipeline_v1/test_bin_dataset\",\n",
    "    label_counts=test_bin_label_counts,\n",
    "    class_names=test_bin_label_names,\n",
    "    label_method=Image_Label_Method.BINARY,\n",
    "    validation_frac=0.2,\n",
    "    test_frac=0,\n",
    "    description=\"First example with 50 values from each class and binary labeling\"\n",
    ")\n",
    "\n",
    "test_bin_ds = Image_Dataset_Builder(db_engine, test_bin_config)\n",
    "test_bin_ds.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splits assigned\n",
      "INFO:root:Database cleared\n",
      "INFO:root:Directory cleared\n",
      "INFO:root:Images copied\n",
      "INFO:root:Data stored to db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_multi_label_counts = {pair: 50 for pair in unique_src_label_pairs}\n",
    "test_multi_label_names = {\n",
    "    ('negative', 'Google Images'): \"negative_similar_plant\",\n",
    "    ('negative', 'Imagenet'): \"negative_random_picture\",\n",
    "    ('negative', 'Plantnet'): \"negative_general_plant\",\n",
    "    ('positive', 'Google Images'): \"positive\",\n",
    "}\n",
    "test_multi_config = Image_Dataset_Config(\n",
    "    name=\"test_multi_example\",\n",
    "    target_dir=\"../datasets/pipeline_v1/test_multi_dataset\",\n",
    "    label_counts=test_multi_label_counts,\n",
    "    class_names=test_multi_label_names,\n",
    "    label_method=Image_Label_Method.MULTI,\n",
    "    validation_frac=0.2,\n",
    "    test_frac=0,\n",
    "    description=\"First example with 50 values from each class and multiclass labeling\"\n",
    ")\n",
    "\n",
    "test_multi_ds = Image_Dataset_Builder(db_engine, test_multi_config)\n",
    "test_multi_ds.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splits assigned\n",
      "INFO:root:Database cleared\n",
      "INFO:root:Directory cleared\n",
      "INFO:root:Images copied\n",
      "INFO:root:Data stored to db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_multi_label_counts = {pair: -1 for pair in unique_src_label_pairs}\n",
    "all_multi_label_names = {\n",
    "    ('negative', 'Google Images'): \"negative_similar_plant\",\n",
    "    ('negative', 'Imagenet'): \"negative_random_picture\",\n",
    "    ('negative', 'Plantnet'): \"negative_general_plant\",\n",
    "    ('positive', 'Google Images'): \"positive\",\n",
    "}\n",
    "all_multi_config = Image_Dataset_Config(\n",
    "    name=\"all_v1_multiclass\",\n",
    "    target_dir=\"../datasets/pipeline_v1/all_v1_multiclass\",\n",
    "    label_counts=all_multi_label_counts,\n",
    "    class_names=all_multi_label_names,\n",
    "    label_method=Image_Label_Method.MULTI,\n",
    "    validation_frac=0.2,\n",
    "    test_frac=0,\n",
    "    description=\"All images in v1 dataset with multiclass labels\"\n",
    ")\n",
    "\n",
    "all_multi_ds = Image_Dataset_Builder(db_engine, all_multi_config)\n",
    "all_multi_ds.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n",
      "1934\n",
      "1762\n",
      "3138\n",
      "441\n",
      "483\n",
      "441\n",
      "785\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/train/positive\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/train/negative_general_plant\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/train/negative_similar_plant\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/train/negative_random_picture\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/validation/positive\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/validation/negative_general_plant\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/validation/negative_similar_plant\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_multiclass/validation/negative_random_picture\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Splits assigned\n",
      "INFO:root:Database cleared\n",
      "INFO:root:Directory cleared\n",
      "INFO:root:Images copied\n",
      "INFO:root:Data stored to db\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_binary_label_counts = {pair: -1 for pair in unique_src_label_pairs}\n",
    "all_binary_label_names = {\n",
    "    ('negative', 'Google Images'): \"negative_similar_plant\",\n",
    "    ('negative', 'Imagenet'): \"negative_random_picture\",\n",
    "    ('negative', 'Plantnet'): \"negative_general_plant\",\n",
    "    ('positive', 'Google Images'): \"positive\",\n",
    "}\n",
    "all_binary_config = Image_Dataset_Config(\n",
    "    name=\"all_v1_binary\",\n",
    "    target_dir=\"../datasets/pipeline_v1/all_v1_binary\",\n",
    "    label_counts=all_binary_label_counts,\n",
    "    class_names=all_binary_label_names,\n",
    "    label_method=Image_Label_Method.BINARY,\n",
    "    validation_frac=0.2,\n",
    "    test_frac=0,\n",
    "    description=\"All images in v1 dataset with binary\"\n",
    ")\n",
    "\n",
    "all_binary_ds = Image_Dataset_Builder(db_engine, all_binary_config)\n",
    "all_binary_ds.build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n",
      "6834\n",
      "441\n",
      "1709\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_binary/train/positive\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_binary/train/negative\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_binary/validation/positive\")))\n",
    "print(len(os.listdir(\"../datasets/pipeline_v1/all_v1_binary/validation/negative\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
